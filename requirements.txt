packaging
wheel
torch>=1.13.1
transformers>=4.36.0
datasets>=2.12.0
accelerate>=0.21.0
peft>=0.4.0
trl==0.7.2
gradio>=4.19.2
scipy
sentencepiece
protobuf
tiktoken
fire
jieba
rouge-chinese
nltk
uvicorn
pydantic
fastapi
sse-starlette
matplotlib
py-cpuinfo
deepspeed
bitsandbytes>=0.39.0
wandb
pandas
openai
langchain<0.0.312,>=0.0.311
rich
pandas-stubs
types-tqdm
logzero
gin-config
absl-py
names
together
sotopia
#flash-attn need to pip install flash-attn --no-build-isolation