stage: sft
model_name_or_path: NEED-CONFIG
dataset: dummy_convs
dataset_dir: ../llm_rl/data/
cutoff_len: 128
template: llama2-sotopia
wandb_project: NEED-CONFIG
wandb_tags: "[]"
use_fast_tokenizer: False
do_train: true
num_train_epochs: 20.0
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
finetuning_type: lora
lora_target: q_proj,v_proj
qlora_compute_dtype: bf16
learning_rate: 5e-5
lr_scheduler_type: cosine
weight_decay: 0.
warmup_ratio: 0.03
quantization_bit: 4
quantization_type: nf4
double_quantization: True
flash_attn: True
gradient_checkpointing: True
bf16: True
cache_dir: ./model_cache
overwrite_cache: true
output_dir: ./output_cache
overwrite_output_dir: true
logging_steps: 1
save_strategy: "epoch"
save_total_limit: 5
use_auth_token: True
wandb_token: NEED-CONFIG
hf_auth_token: NEED-CONFIG
deepspeed: ../llm_rl/deepspeed_config_s2.json
